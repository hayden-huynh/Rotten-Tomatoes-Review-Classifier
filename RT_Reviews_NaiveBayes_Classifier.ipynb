{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1Tdr3KkQh1eHnVV-Urozb1278pv-4AeKg",
      "authorship_tag": "ABX9TyOAOQoz2jr4mEJ/YE7LwOF6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hayden-huynh/Rotten-Tomatoes-Review-Classifier/blob/master/RT_Reviews_NaiveBayes_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Process"
      ],
      "metadata": {
        "id": "s0NCiDbRKgnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download the Rotten Tomatoes Reviews [dataset](https://www.kaggle.com/datasets/ulrikthygepedersen/rotten-tomatoes-reviews)\n",
        "2. Text data pre-processing:\n",
        "  - Lower-casing\n",
        "  - Punctuation Removal\n",
        "  - Tokenization\n",
        "3. Split the original dataset into smaller *train* (70%), *dev* (10%), and *test* (20%) datasets\n",
        "4. Train the classifier using *train* dataset\n",
        "  - Calculate and store P(fresh) and P(rotten) priors\n",
        "  - Calculate and store likelihoods of words\n",
        "5. Improve the classifier using *dev* dataset\n",
        "  - Smoothing\n",
        "  - Float Probability vs Log Probability\n",
        "  - Stemming and Lemmatization (?)\n",
        "6. Test accuracy of the classifier using *test* dataset "
      ],
      "metadata": {
        "id": "qP5EK7aoK022"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset from Kaggle"
      ],
      "metadata": {
        "id": "ITGWYwb3Mw3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Rotten Tomatoes Reviews dataset from Kaggle\n",
        "# Reference 1 (Ref 1): https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/\n",
        "\n",
        "# Ref 1 starts =====\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download ulrikthygepedersen/rotten-tomatoes-reviews\n",
        "! unzip rotten-tomatoes-reviews.zip\n",
        "# ===== Ref 1 ends"
      ],
      "metadata": {
        "id": "GF-mv6wrLHSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99913b9c-f80b-4880-e381-3e604e0f858d"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (3.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "rotten-tomatoes-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  rotten-tomatoes-reviews.zip\n",
            "replace rt_reviews.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: rt_reviews.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Pre-processing"
      ],
      "metadata": {
        "id": "ge7uGOzb7cyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference 2 (Ref 2): https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "reviews = pd.read_csv(\"/content/rt_reviews.csv\", encoding=\"latin-1\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"----- Samples -----\")\n",
        "print(reviews.head(5))\n",
        "\n",
        "print(\"\\n----- Summary -----\")\n",
        "print(reviews.describe())\n",
        "\n",
        "print(\"\\n----- NLTK Stopwords -----\")\n",
        "nltk_stop_words = stopwords.words('english')\n",
        "stop_words = ['the','a','and','of','is','to','it','in','that','its']\n",
        "stop_words += ['with','but','this','for','as','an','on','be']\n",
        "stop_words += ['film','movie','not','you','at','by','from','are','has','more','like','than']\n",
        "stop_words += ['one','about','his','all','if','have','so','or']\n",
        "stop_words += ['story','what','into','just','up','even']\n",
        "stop_words += ['i','good','films','some','out']\n",
        "# stop_words += ['much','was','which','who','will','can','time','their','too','only','there','doesnt','make']\n",
        "# stop_words += ['full','way','while','when','makes','been','characters','comedy','most','any','theres']\n",
        "# stop_words += ['no','life','review','director','movies','us','feels','enough','would','they','isnt']\n",
        "# stop_words += ['may','he','work','also','could','really','thats','action','how']\n",
        "# stop_words += ['very','we','drama','still','get','here','plot','do']\n",
        "# stop_words += ['spanish','performance','performances','might','many','nothing','two','something','should']\n",
        "# stop_words += ['her','your','made','cant','them','off','does','first','never','little']\n",
        "# stop_words += ['½ï','both','see','seems','being','through','script','over','dont']\n",
        "# stop_words += ['new','world','those','end','long','funny','well','character','without']\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6l8UQu37cF-",
        "outputId": "f1f0cf88-c169-4bd9-e3c6-e50fdc4e371e"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Samples -----\n",
            "  Freshness                                             Review\n",
            "0     fresh   Manakamana doesn't answer any questions, yet ...\n",
            "1     fresh   Wilfully offensive and powered by a chest-thu...\n",
            "2    rotten   It would be difficult to imagine material mor...\n",
            "3    rotten   Despite the gusto its star brings to the role...\n",
            "4    rotten   If there was a good idea at the core of this ...\n",
            "\n",
            "----- Summary -----\n",
            "       Freshness                    Review\n",
            "count     480000                    480000\n",
            "unique         2                    339697\n",
            "top        fresh   Parental Content Review\n",
            "freq      240000                       166\n",
            "\n",
            "----- NLTK Stopwords -----\n",
            "['the', 'a', 'and', 'of', 'is', 'to', 'it', 'in', 'that', 'its', 'with', 'but', 'this', 'for', 'as', 'an', 'on', 'be', 'film', 'movie', 'not', 'you', 'at', 'by', 'from', 'are', 'has', 'more', 'like', 'than', 'one', 'about', 'his', 'all', 'if', 'have', 'so', 'or', 'story', 'what', 'into', 'just', 'up', 'even', 'i', 'good', 'films', 'some', 'out']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower-case all words\n",
        "reviews[\"Review\"] = reviews[\"Review\"].apply(lambda r: r.lower())\n",
        "\n",
        "\n",
        "# Remove Punctuations\n",
        "def remove_punctuations_1(text):\n",
        "  punc_free = \"\".join([char for char in text if char not in string.punctuation])\n",
        "  return punc_free\n",
        "\n",
        "def remove_punctuations_2(words):\n",
        "  for i, w in enumerate(words):\n",
        "    punc_free = \"\".join([char for char in w if char not in string.punctuation])\n",
        "    words[i] = punc_free\n",
        "  return words\n",
        "\n",
        "reviews[\"Review_tokens\"] = reviews[\"Review\"].apply(lambda r: remove_punctuations_1(r))\n",
        "\n",
        "# nltk_stop_words = list(map(lambda r: remove_punctuations_1(r), nltk_stop_words))\n",
        "# stop_words += nltk_stop_words\n",
        "\n",
        "\n",
        "# Tokenizing and Removing Duplicate Words\n",
        "def tokenize(text):\n",
        "  tokens = re.split(\"\\W+\", text)\n",
        "  tokens = list(filter(None, tokens))\n",
        "  return sorted(list(set(tokens)))\n",
        "\n",
        "reviews[\"Review_tokens\"] = reviews[\"Review_tokens\"].apply(lambda r: tokenize(r))\n",
        "\n",
        "\n",
        "# Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize(words):\n",
        "  lemm_words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
        "  return list(set(lemm_words))\n",
        "\n",
        "reviews[\"Review_tokens\"] = reviews[\"Review_tokens\"].apply(lambda words: lemmatize(words))\n",
        "\n",
        "\n",
        "# Remove stopwords\n",
        "def remove_stopwords(words):\n",
        "  output = [w for w in words if w not in stop_words]\n",
        "  return output\n",
        "\n",
        "reviews[\"Review_tokens\"] = reviews[\"Review_tokens\"].apply(lambda words: remove_stopwords(words))\n",
        "\n",
        "\n",
        "reviews.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "jmmTfKN08SXA",
        "outputId": "63e1008f-483f-4731-a340-d1b40b5b5057"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Freshness                                             Review  \\\n",
              "432675     fresh   the best thing about the ultimately involving...   \n",
              "117502    rotten   a typically underwhelming effort from david o...   \n",
              "65329      fresh   exactly the biopic barnum would've made about...   \n",
              "188852     fresh   it's enjoyable and fun to watch, but just bar...   \n",
              "46632     rotten   at least with the jackass films you could sen...   \n",
              "\n",
              "                                            Review_tokens  \n",
              "432675  [make, outcome, already, best, end, look, enou...  \n",
              "117502  [underwhelming, effort, russell, typically, da...  \n",
              "65329   [forget, barnum, wouldve, aisle, biopic, way, ...  \n",
              "188852  [own, watch, enjoyable, barely, fun, surface, ...  \n",
              "46632   [work, could, roller, broken, down, poorly, se...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b86ed45-b658-4b4e-8c9e-c323418f7815\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Freshness</th>\n",
              "      <th>Review</th>\n",
              "      <th>Review_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>432675</th>\n",
              "      <td>fresh</td>\n",
              "      <td>the best thing about the ultimately involving...</td>\n",
              "      <td>[make, outcome, already, best, end, look, enou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117502</th>\n",
              "      <td>rotten</td>\n",
              "      <td>a typically underwhelming effort from david o...</td>\n",
              "      <td>[underwhelming, effort, russell, typically, da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65329</th>\n",
              "      <td>fresh</td>\n",
              "      <td>exactly the biopic barnum would've made about...</td>\n",
              "      <td>[forget, barnum, wouldve, aisle, biopic, way, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188852</th>\n",
              "      <td>fresh</td>\n",
              "      <td>it's enjoyable and fun to watch, but just bar...</td>\n",
              "      <td>[own, watch, enjoyable, barely, fun, surface, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46632</th>\n",
              "      <td>rotten</td>\n",
              "      <td>at least with the jackass films you could sen...</td>\n",
              "      <td>[work, could, roller, broken, down, poorly, se...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b86ed45-b658-4b4e-8c9e-c323418f7815')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b86ed45-b658-4b4e-8c9e-c323418f7815 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b86ed45-b658-4b4e-8c9e-c323418f7815');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the Original Dataset"
      ],
      "metadata": {
        "id": "mYMdGlcqM2xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference 3 (Ref 3): https://stackoverflow.com/questions/43777243/how-to-split-a-dataframe-in-pandas-in-predefined-percentages \n",
        "\n",
        "# Ref 3 starts =====\n",
        "def split_by_fractions(df, fracs, random_state=0):\n",
        "    remain = df.index.copy().to_frame()\n",
        "    res = []\n",
        "    for i in range(len(fracs)):\n",
        "        fractions_sum = sum(fracs[i:])\n",
        "        frac = fracs[i]/fractions_sum\n",
        "        idxs = remain.sample(frac=frac, random_state=random_state).index\n",
        "        remain=remain.drop(idxs)\n",
        "        res.append(idxs)\n",
        "    return [df.loc[idxs] for idxs in res]\n",
        "# Ref 3 ends =====\n",
        "\n",
        "random_state = 1\n",
        "train, dev, test = split_by_fractions(reviews, [0.7, 0.1, 0.2], random_state)\n",
        "print(train.shape, dev.shape, test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zt9wPa0NELt",
        "outputId": "853696fe-f478-422f-ba62-c2f44904ffc2"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(336000, 3) (48000, 3) (96000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Ae1dhZ5fv1xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from decimal import Decimal\n",
        "\n",
        "train_fresh = train.loc[train[\"Freshness\"] == \"fresh\"]\n",
        "train_rotten = train.loc[train[\"Freshness\"] == \"rotten\"]\n",
        "\n",
        "# P(fresh) and P(rotten) priors\n",
        "p_fresh = Decimal(len(train_fresh) / len(train))\n",
        "p_rotten = Decimal(len(train_rotten) / len(train))\n",
        "\n",
        "print(f'P(fresh) = {p_fresh}')\n",
        "print(f'P(rotten) = {p_rotten}')"
      ],
      "metadata": {
        "id": "JOz9w1SrOt8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cd19e9-fd6d-4a13-8351-44c2152c858f"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(fresh) = 0.50085119047619042209618100969237275421619415283203125\n",
            "P(rotten) = 0.499148809523809522392667759049800224602222442626953125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count word occurences\n",
        "occ_fresh = {}\n",
        "occ_rotten = {}\n",
        "\n",
        "for words in train_fresh.loc[:,\"Review_tokens\"]:\n",
        "  for w in words:\n",
        "    if w not in occ_fresh.keys():\n",
        "      occ_fresh[w] = 1\n",
        "    else:\n",
        "      occ_fresh[w] += 1\n",
        "\n",
        "for words in train_rotten.loc[:,\"Review_tokens\"]:\n",
        "  for w in words:\n",
        "    if w not in occ_rotten.keys():\n",
        "      occ_rotten[w] = 1\n",
        "    else:\n",
        "      occ_rotten[w] += 1"
      ],
      "metadata": {
        "id": "PmvgE_Gv2DDf"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word probabilities given fresh or rotten\n",
        "probs_fresh = {}\n",
        "probs_rotten = {}\n",
        "\n",
        "def calc_word_likelihood(count, alpha, h):\n",
        "  if h == \"fresh\":\n",
        "    return Decimal((count + alpha) / (len(train_fresh) + alpha * len(occ_fresh)))\n",
        "  elif h == \"rotten\":\n",
        "    return Decimal((count + alpha) / (len(train_rotten) + alpha * len(occ_rotten)))\n",
        "\n",
        "def calc_prob(alpha=0):\n",
        "  for word, count in occ_fresh.items():\n",
        "    probs_fresh[word] = calc_word_likelihood(count, alpha, \"fresh\")\n",
        "  \n",
        "  for word, count in occ_rotten.items():\n",
        "    probs_rotten[word] = calc_word_likelihood(count, alpha, \"rotten\")\n",
        "\n",
        "alpha = 1\n",
        "\n",
        "calc_prob(alpha)\n",
        "\n",
        "\n",
        "\n",
        "# Derive top 10 most influential words for each class\n",
        "\n",
        "probs_fresh_sorted = dict(sorted(probs_fresh.items(), key=lambda item: item[1], reverse=True))\n",
        "probs_rotten_sorted = dict(sorted(probs_rotten.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "print(\"----- Fresh Top 10 -----\")\n",
        "i = 0\n",
        "for key, value in probs_fresh_sorted.items():\n",
        "  print(f\"{key}: {value}\")\n",
        "  i += 1\n",
        "  if i == 10:\n",
        "    break\n",
        "\n",
        "print(\"\\n----- Rotten Top 10 -----\")\n",
        "j = 0\n",
        "for key, value in probs_rotten_sorted.items():\n",
        "  print(f\"{key}: {value}\")\n",
        "  j += 1\n",
        "  if j == 10:\n",
        "    break"
      ],
      "metadata": {
        "id": "NlZEAjmw9Gg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90bd53a2-cc10-489b-d8fe-33374de0515f"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Fresh Top 10 -----\n",
            "ha: 0.0465814100097428351876516217089374549686908721923828125\n",
            "make: 0.038643862421299975118405001239807461388409137725830078125\n",
            "most: 0.034124495460164890048293528934664209373295307159423828125\n",
            "performance: 0.033690570733824025395630741286367992870509624481201171875\n",
            "there: 0.032986466460893558438893791162627167068421840667724609375\n",
            "who: 0.031762471242252808545369902049060328863561153411865234375\n",
            "time: 0.0303256072899354030270036020056068082340061664581298828125\n",
            "character: 0.030034959595876896398802813337169936858117580413818359375\n",
            "will: 0.028692249121916472975879486284611630253493785858154296875\n",
            "best: 0.0274641602737819394419727103695549885742366313934326171875\n",
            "\n",
            "----- Rotten Top 10 -----\n",
            "there: 0.0448562471038602694761721068061888217926025390625\n",
            "ha: 0.04412568933343909149957795534646720625460147857666015625\n",
            "much: 0.03860267258905498988230675649901968427002429962158203125\n",
            "make: 0.0370998108899028566920463845235644839704036712646484375\n",
            "too: 0.037016318573283295279008342504312167875468730926513671875\n",
            "character: 0.035321424545906164593755960368071100674569606781005859375\n",
            "feel: 0.031514174908054089219167082092099008150398731231689453125\n",
            "time: 0.029447740071719900878388642695426824502646923065185546875\n",
            "only: 0.0293433746759454473773676141945543349720537662506103515625\n",
            "no: 0.029322501596790555289384627712934161536395549774169921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with *dev* dataset"
      ],
      "metadata": {
        "id": "OO9Y1-kVRHz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Function to classify a review\n",
        "def classify(review_words, alpha):\n",
        "  for w in review_words:\n",
        "    if w not in probs_fresh.keys():\n",
        "      probs_fresh[w] = calc_word_likelihood(0, alpha, \"fresh\")\n",
        "    if w not in probs_rotten.keys():\n",
        "      probs_rotten[w] = calc_word_likelihood(0, alpha, \"rotten\")\n",
        "  \n",
        "  chance_fresh = p_fresh\n",
        "  chance_rotten = p_rotten\n",
        "  for w in review_words:\n",
        "    chance_fresh = chance_fresh * probs_fresh[w]\n",
        "    chance_rotten = chance_rotten * probs_rotten[w]\n",
        "  \n",
        "  if chance_fresh > chance_rotten:\n",
        "    return \"fresh\"\n",
        "  else:\n",
        "    return \"rotten\"\n",
        "\n",
        "\n",
        "# Function to classify a review, log10 applied to avoid underflowing floats\n",
        "def classify_log(review_words, alpha):\n",
        "  for w in review_words:\n",
        "    if w not in probs_fresh.keys():\n",
        "      probs_fresh[w] = calc_word_likelihood(0, alpha, \"fresh\")\n",
        "    if w not in probs_rotten.keys():\n",
        "      probs_rotten[w] = calc_word_likelihood(0, alpha, \"rotten\")\n",
        "\n",
        "  chance_fresh = p_fresh.log10()\n",
        "  chance_rotten = p_rotten.log10()\n",
        "  for w in review_words:\n",
        "    chance_fresh = chance_fresh + probs_fresh[w].log10()\n",
        "    chance_rotten = chance_rotten + probs_rotten[w].log10()\n",
        "\n",
        "  if chance_fresh > chance_rotten:\n",
        "    return \"fresh\"\n",
        "  else:\n",
        "    return \"rotten\"\n",
        "\n",
        "\n",
        "# Function to test entire dataset given\n",
        "def test_accuracy(dataset, alpha, use_log=False, csv_writer=None):\n",
        "  correct = 0\n",
        "  \n",
        "  for index, row in dataset.loc[:,[\"Freshness\", \"Review_tokens\"]].iterrows():\n",
        "    result = \"\"\n",
        "\n",
        "    if use_log:\n",
        "      result = classify_log(row[\"Review_tokens\"], alpha)\n",
        "    else:\n",
        "      result = classify(row[\"Review_tokens\"], alpha)\n",
        "    \n",
        "    if row[\"Freshness\"] == result:\n",
        "      correct += 1\n",
        "  \n",
        "  accuracy = round(correct / len(dataset) * 100, 4)\n",
        "\n",
        "  if csv_writer != None:\n",
        "    csv_writer.writerow([alpha, accuracy])\n",
        "  \n",
        "  print(f\"Successfully classified {correct}/{len(dataset)} ({accuracy}%) correctly\")\n",
        "\n",
        "\n",
        "\n",
        "# Experiment with Smoothing\n",
        "\n",
        "# dev_smoothing = open(f\"dev_smoothing_log.csv\", \"a\", newline='')\n",
        "# dev_smoothing_writer = csv.writer(dev_smoothing)\n",
        "# if (os.path.getsize(f\"/content/dev_smoothing_log.csv\") == 0):\n",
        "#   dev_smoothing_writer.writerow([\"alpha\", \"accuracy\"])\n",
        "\n",
        "test_accuracy(dev, alpha)\n",
        "# dev_smoothing.flush()"
      ],
      "metadata": {
        "id": "owvJOQIdD_h6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd557a84-3a6a-4e06-c209-b68f974b8f7b"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully classified 38561/48000 (80.3354%) correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Accuracy with *test* dataset"
      ],
      "metadata": {
        "id": "xAHkH4UiRVzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_accuracy(test, alpha)"
      ],
      "metadata": {
        "id": "YbeR39pwRZR8"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "th2LxLMR-6wP"
      },
      "execution_count": 244,
      "outputs": []
    }
  ]
}